---
title: "Naive Bayes"
output: html_notebook
---

Vamos a hacer un pequeño modelo estilo naive bayes, más que todo para practicar.

## Bibliotecas a usar

```{r, results = 'hide', collapse = TRUE}
library(tidyverse)
library(caret)
library(e1071)
library(ggcorrplot)
library(pROC)
```

## Cargar los datos

```{r}
df = read.csv("../data/datos_eda.csv", sep = ';')
head(df)
names(df)
```
Observamos que id es una variable que no queremos usar.

```{r}
df <- df %>% select(-id) %>% mutate(diagnostico_pro = factor(diagnostico_pro))
```


## Modelo

El modelo que vamos a buscar plantear uno que este diagnostico_pro como variable respuesta en función de las otras. Como tenemos la restricción de independencia de las variables predictoras vamos a realizar un heatmap entre las variables numéricas.

```{r}
var_numericas <- c("d1",  "d2", "d3", "d4", "d5", "edad", "paquetes_por_dia", "anios_fumo" )

df %>% 
  select(all_of(var_numericas)) %>% 
  cor(method = "pearson") %>% 
  ggcorrplot(lab = TRUE)
```

vemos que hay problemas con las variables paquetes_por_dia y anios_fumo. Sera por la cantidad de NAs?

```{r, collapse = TRUE}
df %>% 
  select(all_of(var_numericas)) %>% 
  map_int(~ sum(is.na(.)))

```

Con esto llegamos a la conclusión que es problema son los NAs, entonces los exlcuimos.

```{r}
var_numericas <- var_numericas[-c(7, 8)]

df %>% 
  select(all_of(var_numericas)) %>% 
  cor(method = "pearson") %>% 
  ggcorrplot(lab = TRUE)
```

No observamos que exista variables altamente correlacionada. 

Además, deberemos verificar que su distrbución sea, o almenos aproximadamente, normal.

```{r}
df %>% 
  select(all_of(var_numericas)) %>% 
  map(~ ggplot(df, aes(.)) + geom_density())
```

Observamos que, exceptuando d3, d5 y edad, se acercan bastante a las normalidad. Probamos una transformación en estas variables. **¿Es posible que este modelo sea robusto a la falta de normalidad?**

```{r}
df %>% 
  select(d3, d5, edad) %>% 
  mutate(ln_d3 = log(d3, exp(1)),
         ln_d5 = log(d5, exp(1)),
         ln_edad = log(edad, exp(1))) %>% 
  ggplot() +
  geom_density(aes(ln_d3), color = "red") +
  geom_density(aes(ln_d5), color = "blue") +
  geom_density(aes(ln_edad), color = "green") 
```

Observamos que las varibles adpotan una forma más semejante a una normal con esta transformación.

```{r df2}
df2 <- df %>% 
  mutate(ln_d3 = log(d3, exp(1)),
         ln_d5 = log(d5, exp(1)),
         ln_edad = log(edad, exp(1)))

var_numericas <- c("d1",  "d2", "ln_d3", "d4", "ln_d5", "ln_edad")
```

Para las variables categóricas vamos a utilizar

```{r}
var_categ <- c("educ", "empleo",  "genero",  "estado_marital",  "facilidad_celular", "fumo")
```

**¿Cómo se podría ver si son independientes las va categóricas?**

```{r split_datos}
# Nos quedamos con las vaiables que queremos

variables <- c(var_numericas, var_categ, "diagnostico_pro")

df_bayes <- df2 %>% 
  select(all_of(variables))

# Dividimos el dataste

set.seed(100)
indices_train <- createDataPartition(df_bayes$diagnostico_pro, p = .7, list = FALSE)
datos_train <- df_bayes[indices_train, ]
datos_test <- df_bayes[-indices_train, ]

# Creamos el modelo

modelo_bayes <- naiveBayes(diagnostico_pro ~ ., data = datos_train)
modelo_bayes
```

Ahora que tenemos el modelo, comenzamos realizando las predicciones

```{r roc, collapse = TRUE}
predicciones_bayes <- predict(modelo_bayes, datos_test, type = "raw")
objeto_roc <- roc(datos_test$diagnostico_pro, predicciones_bayes[,"true"])

data.frame(sensitivity = objeto_roc$sensitivities,
           specificity = objeto_roc$specificities) %>% 
  ggplot(aes(1 - specificity, sensitivity)) +
  geom_line()

auc(objeto_roc)
```

Observamos que este modelo consigue un buen valor de AUC. Para seguir, vamos a armar rapidamente una regresión logística usando las mismas variables, para luego comparar ambos modelos.

**TODO: Hacer regresión logística**